# 5th of July
For the development of the NN and the system in general it han been deduced **three** lines of investigation:

```bash
*NN ---¬
       |> Improve NN 
       |          |----> 1.  Rebuild and optimize the image
       |          |----> 2.  Seek for NN optimization (GPU, libraries)
       |
       |> Train the NN
                  |----> 3.  Change NN classes, only 3 (extinguisher, door, exit sign)
                  |----> 4.  Increase the dataset
                  |----> 5.  Retrain until a better performance

* System performance ¬
                     |-> 6.  Solution with two pipelines
                     |-> 7.  Solution using the 3s and object traking between frames

* New Features ---¬
                   |---> 8.  How to perform object tracking
                   |---> 9.  How to meassure distances between objects
                   |---> 10. How to compute rotation angle
                   |---> 11. How to draw features in the map
       
```

## 7. Swith between object detection and object traking.
The solution is over the facts that:
* We use only 50% of RPi CPU running the NN
* The NN inference time is higly related with the convolutional layers. Because of the poor integration of Tensorflow with RPi, no          GPU is used, what would produce an insane time to process each image: **3 seconds**.
* We have found that OpenCV is more optimized that tensorflow. This fact in conjunction with the reduce processing time of object tracking (compare to object detection) has give us a partial solution: use tracking between detection.

**Benefits:**
* We can achieve a better response time
* It is very usefull for computing angles and distances!!!!
https://www.learnopencv.com/object-tracking-using-opencv-cpp-python/

## 6. Two pipelines processing
A partial but not very usefull solution is to use **two pipelines** and **multithreading**. We take advantage of the 50% of the CPU usage to run multiple instances. The problem is that although we can increase the throughput, the latency will remain the same.


